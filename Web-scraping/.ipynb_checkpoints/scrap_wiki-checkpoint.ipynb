{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "uni_names = pd.read_csv(\"./universities.txt\", header=None)\n",
    "\n",
    "uni_names.columns = ['name']\n",
    "\n",
    "# initialize a pandas dataframe that will contain all the scraped data\n",
    "# add more columns name as needed\n",
    "data = pd.DataFrame(columns=['name', \"locality\", \"state\", \"country\", 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>locality</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>University of Toronto</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>Canada</td>\n",
       "      <td>https://utoronto.ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>University of York</td>\n",
       "      <td>Heslington, York</td>\n",
       "      <td>NaN</td>\n",
       "      <td>England</td>\n",
       "      <td>https://www.york.ac.uk/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harvard University</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>United States</td>\n",
       "      <td>http://harvard.edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yale University</td>\n",
       "      <td>New Haven</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>United States</td>\n",
       "      <td>http://yale.edu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name          locality          state        country  \\\n",
       "0  University of Toronto           Toronto        Ontario         Canada   \n",
       "1     University of York  Heslington, York            NaN        England   \n",
       "2     Harvard University         Cambridge  Massachusetts  United States   \n",
       "3        Yale University         New Haven    Connecticut  United States   \n",
       "\n",
       "                       url  \n",
       "0      https://utoronto.ca  \n",
       "1  https://www.york.ac.uk/  \n",
       "2       http://harvard.edu  \n",
       "3          http://yale.edu  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for uni in uni_names.name:\n",
    "    # make url -> fetch page -> parse html -> select the table on the sidebar\n",
    "    uni_wiki_url = \"https://en.wikipedia.org/wiki/\" + \"_\".join(uni.split(\" \"))\n",
    "    page = requests.get(uni_wiki_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    sidebar_table = soup.find_all('table', {\"class\": \"infobox vcard\"})[0]\n",
    "    \n",
    "    # key-value variable to store everything for one particular uni\n",
    "    juice = {\"name\" : uni}\n",
    "\n",
    "    # find url\n",
    "    url = sidebar_table.find(\"span\", class_=\"url\")\n",
    "    url = url.find(\"a\", href=True)\n",
    "    juice['url'] = url['href']\n",
    "    \n",
    "    try:\n",
    "        queries = ['locality', 'country-name', 'state']\n",
    "        columns = ['locality', 'country', 'state']\n",
    "        for query, column in zip(queries, columns):\n",
    "            location = sidebar_table.find(\"th\", string=\"Location\").parent\n",
    "            loc = location.find(\"div\", class_=query)\n",
    "            juice[column] = loc.text\n",
    "    except:\n",
    "        # probably, state was missing\n",
    "        pass\n",
    "    # Add the new Uni entry to the grand table aka. `data`\n",
    "    data = data.append(juice, ignore_index=True)  \n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# url = sidebar_table.find_all(\"span\", class_=\"url\")[0]\n",
    "# url = url.find(\"a\", href=True)\n",
    "# url['href']\n",
    "\n",
    "# th = sidebar_table.find_all(\"th\", string='Location')[0]\n",
    "# th.parent.td.find('div', class_='locality')\n",
    "\n",
    "#     lookup_text = ['Location', 'Website']  # name of relevant row in the sidebar on wiki\n",
    "#     column_names = ['location', 'url']     # corresponding header/column name to be used in DataFrame\n",
    "    \n",
    "#     for text, column in zip(lookup_text, column_names):\n",
    "#         # lookup the info and store it in juice with corresponding column name\n",
    "#         th = sidebar_table.find_all(\"th\", string=text)[0]\n",
    "#         juice[column] = th.parent.td.text\n",
    "        \n",
    "#         url = sidebar_table.find_all(\"span\", class_=\"url\")[0]\n",
    "#         url = url.find(\"a\", href=True)\n",
    "#         juice['url'] = url['href']\n",
    "    \n",
    " \n",
    "#     # get rid of comments, scripts, styles from html source\n",
    "#     blacklisted = [\"script\", \"style\", \"comment\"]\n",
    "#     for tag in soup.findAll():\n",
    "#             if tag.name.lower() in blacklisted:\n",
    "#                 tag.extract()\n",
    "\n",
    "\n",
    "# for tr in sidebar_table.tbody:\n",
    "#     try:\n",
    "#         if tr.th.text == 'Website':\n",
    "#             juice[\"url\"] = tr.td.text\n",
    "#         elif tr.th.text == 'Location':\n",
    "#             juice[\"location\"] = tr.td.text\n",
    "#     except:\n",
    "#         # empty table row/header\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_ranking_url = \"https://www.4icu.org/top-universities-world/\"\n",
    "\n",
    "page = requests.get(uni_ranking_url)\n",
    "soup_ranking = BeautifulSoup(page.content, 'html.parser')\n",
    "universities = []\n",
    "table = soup_ranking.find(\"table\", class_=\"table table-hover\")\n",
    "for tr in table.find_all('tr')[1:]:\n",
    "    universities.append(tr.find_all(\"td\")[1].text)\n",
    "\n",
    "universities = pd.DataFrame(universities)\n",
    "universities.to_csv(\"universities.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
